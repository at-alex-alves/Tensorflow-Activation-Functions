# Tensorflow Activation Functions

Implementation of all Tensorflow's built-in activation functions and its plots. 

The activation functions are:

- Deserialize
- Elu (Exponential Linear Unit)
- Exponential
- Gelu (Gaussian Error Linear Unit)
- Get
- Hard Sigmoid
- Linear (or Identity)
- Relu (Rectified linear unit)
- Selu
- Serialize
- Sigmoid (or Logistic Activation Function)
- Softmax
- Softplus
- Softsign
- Swish
- Tahn (or Hyperbolic tangent)

Based on https://www.tensorflow.org/api_docs/python/tf/keras/activations
